Chapter 2. OpenStack 오브젝트 스토리지 소개

내용

2.1. 어카운트와 어카운트 서버
2.2. 인증과 접근 허가
2.3. 컨테이너와 오브젝트
2.4. 오퍼레이션
2.5. Language-Specific API Bindings

OpenStack Object Storage is a scalable object storage system - it is not a file system in the traditional sense.
You will not be able to mount this system like traditional SAN or NAS volumes. 
Since OpenStack Object Storage is a different way of thinking when it comes to storage, 
take a few moments to review the key concepts listed below.

2.1. 어카운트와 어카운트 서버

OpenStack 오브젝트 스토리지 시스템은 소비자또는 많은 다른 스토리지 사용자에 의해 사용되기 위해서 설계되었다.
각 사용자는 인증시스템을 사용해서 그들을 식별해야 하고, 허가(권한) 시스템은 OpenStack 오브젝트 스토리지 (swauth) 를 위해 제공된다.

어카운트 서비스를 실행하는 노드들은 각각의 어카운트로 부터 분리된 개념이다.
어카운트 서버는 스토리지 시스템의 부분이고 컨테이너 서버와 오브젝트서버와 함께 구성되어야 한다.

2.2. 인증과 접근 허가
OpenStack 오브젝트 스토리지 시스템 커넥션 파라미미터와 인증토큰을 받기 위해 인증 서비스에 대해 인증을 해야한다.
토큰은 모든 subsequent 컨테이너/오브젝트 오퍼레이션을 위해 안으로 건네져야한다.
OpenStack 오브젝트 스토리지는 swauth 라 불리는 미들웨어 예제로 사용할수 있는것을 현재 인증서비스로 제공한다.

Note
통상 language-specifiec API 는 토큰 패스, 와 HTTPS 요청/응답 통신을 핸들한다.

읽기위한 accountname 어카운트로 부터 어떠한 사용자를 허가하는 하지만 쓰기를 위한 accountname 어카운트로 부터 username 사용자를 오직 허가하는
X-Container-Read accountname 과 X-Container-Write: accountname:username 를 사용해서 사용자와 어카운트를 와 오브젝트를 위한 접근 제어를 구현할수 있다.

OpenStack 오브젝트 스토리지내에 저장된 오브젝트로 공개접근을 또한 승인할수 있다. 하지만 hot-linking 과 같은 site-based 컨텐츠 절도를 예방하기 위해서
Referer 헤더를 사용해서 공개접근을 제한할수 있다.
(예를들면, off-site 에서 이미지 파일로 연결 과 반면 다른것의 대역폭을 사용해서 ??? )

public 컨테이너 설정은 접근 제어 목록을 거쳐 디폴트 허가로 사용된다.
예를들면, X-Container-Read: referer:any 를 사용해서 다른 허가설정의 개의치 않은 컨테이너에서 읽기 위한 anyone 를 허가한다.

일반적으로 말하면, 각 사용자는 그들의 스토리지 어카운트를 가지고, 그 어카운트에서 전체 접근을 가진다.
사용자는 위에서 기술된것 처럼 그들의 credentials 로 인증을 해야하지만,
한번 인증되면 그들은 그 어카운트에서 컨테이너와 오브젝트를 생성/삭제할수 잇다.
사용자가 다른 어카운트에서 컨텐츠를 접근할수 있는 유일한 방법은 만약 그들이 API access 를 공유하거나 그들의 인증시스템에 의해
제공되는 session 토큰이다. ??


2.3. 컨테이너와 오브젝트

컨테이너는 데이터를 조직화하기 위해서 당신을 위한 방법을 제공하고 데이터를 위한 스토리지 공간이다.
UNIX® 내에 디렉토리 또는 Windows® 내에 폴더로서 컨테이너를 생각할수 있다.
컨테이너와 이들 다른 파일시스템 개념사이 첫번째 차이점은 컨테이너는 끼워넣을수 없다는것이다. (둥지를 틀다)
그러나 당신의 어카운트로 컨테이너의 제한없는 수를 생성할수 있다.
데이터는 컨테이너내에 저장되어야 하고 그래서 적어도 업로딩 데이터로 이전의 당신의 어카운트에 정의된 한개의 컨테이너는 가져야 한다.

컨테이너 이름에서 유일한 제한은 그들은 forward slash (/) 를 포함할수 없고, 길이가 256 바이트보다 적어야 한다는것이다.
길이제한은 URL 인코드되어진 이후 이름으로 적용한다는것을 note 하길 바란다.
예를들면, Course Docs 의 컨테이너 이름은 Course%Docs 로 URL 인코드될수 있고, 그때문에 예상된 11 보다 길이내 13 바이트가 될수 있다.
For example, a container name of Course Docs would be URL encoded as Course%20Docs and 
therefore be 13 bytes in length rather than the expected 11.

오브젝트는 기초 스토리지 엔티티이고, OpenStack 오브젝트 스토리지에 저장한 파일을 대신하는 어떤 부가적인 메타데이터이다.
OpenStack 오브젝트 스토리지로 데이터를 업로드할때, 데이터는 as-is ( no compression 또는 encryption ) 과 위치의 구성 ( 컨테이너 ), 
오브젝트 이름, 과 key/value 쌍의 구성한 어떤 메타데이터를 저장된다.
예를들어, 디지털 포토들의 백업을 저정, 앨범으로 그들을 조직화 하기 위해 선택할수도 있다.
이 경우에 각 오브젝트는 앨범과 같은 메터데이터로 테그될수 있다: Caribbean Cruise 또는 Album : Aspen Ski Trip

오브젝트 이름에서 유일한 제한은 URL 인코딩 이후 길이에서 1024 바이트보다 적어야 한다는것이다.
예를들면, C++final(v2).txt 의 오브젝트명 이름은 C%2B%2Bfinal%28v2%29.txt 로 URL 인코드되어야하고
그래서 예상된 16 보다 길이내 24 바이트가 되어야 한다.

업로드 기반 스토리지 오브젝트를 위한 최대 허가할수 있는 크기는 5GB 이고, 최소는 0 바이트이다.
built-in 큰 오브젝트 지원을 사용할수 있고, 5GB 보다 큰 오브젝트를 retrieve (회수,되찾기) 를 하기 위해서 st 유틸리티를 사용할수 있다.

메타데이터 때문에 어떤 한 오브젝트를 위한 개별 key/value 쌍은 90 을 초과되지 않아야하고, 모든 key/value 쌍의 전체 바이트 길이
는 4KB (4096 바이트) 를 초과되지 않아야 한다.


2.4. 오퍼레이션
오퍼레이션은  컨테이너 생성 또는 삭제, 오브젝트 업로드 또는 다운로드등 기타와 같은 OpenStack 오브젝트 스토리지 시스템에서
이행(실행)한 액션이다. 오퍼레이션의 전체 목록은 Developer Guide 에 문서화되어 있다.
오퍼레이션은 ReST web service API 또는 language-specific API 를 경유하여 이행(실행)될수도 있다.; 현재, Python, PHP, Java, Ruby
와 C#/.NET 을 지원한다.

중요
모든 오퍼레이션은 인증시스템에서 유효한 허가 토큰을 포함해야만 한다.


2.5. Language-Specific API Bindings

몇몇 인기있는 언어에서  지원된 API bindings 의 집합은 이 구현을 위한 OpenStack 오브젝트 스토리지 코드를 사용한 Rackspace Cloud Files
제품에서 사용이 가능하다. 이 바인딩들은 base ReST API 의 상위에서 추상계층, 컨테이너와 함께 작업하기 위해 프로그래머를
, HTTP 요청과 응답과 함께 직접적으로 작업하는 대신 오브젝트 모델을 허가하는것을 제공한다.

이 바인딩들은 다운로드, 사용, 수정하는것이 자유롭다 ( as in beer and as in speech ).
이들은 각 바인딩과 함께 패키지된 COPYING 파일내에 기술된것 처럼 MIT 라이센스 아래 모두 라이센스되었다.
만약 API 로 어떤 improvements 를 만든다면, 우리에게 그 뒤로 변경된 그들을 submit 하는것을 장려한다. ( 하지만, 요구사항은 아니다 )

The API bindings for Rackspace Cloud Files are hosted at http://github.com/rackspace. 
Feel free to coordinate your changes through github or, if you prefer, send your changes to cloudfiles@rackspacecloud.com. 
Just make sure to indicate which language and version you modified and send a unified diff.

Each binding includes its own documentation (either HTML, PDF, or CHM). 
They also include code snippets and examples to help you get started. 
The currently supported API binding for OpenStack Object Storage are:

PHP (requires 5.x and the modules: cURL, FileInfo, mbstring)

Python (requires 2.4 or newer)

Java (requires JRE v1.5 or newer)

C#/.NET (requires .NET Framework v3.5)

Ruby (requires 1.8 or newer and mime-tools module)

There are no other supported language-specific bindings at this time. You are welcome to create your own language API bindings and 
we can help answer any questions during development, host your code if you like, and give you full credit for your work.




Chapter 4. OpenStack 오브젝트 스토리지를 위한 시스템 관리

내용

4.1. Understanding How Object Storage Works
4.2. Configuring and Tuning OpenStack Object Storage
4.3. Preparing the Ring
4.4. Server Configuration Reference
4.4.1. Object Server Configuration
4.4.2. Container Server Configuration
4.4.3. Account Server Configuration
4.4.4. Proxy Server Configuration
4.5. Considerations and Tuning
4.5.1. Memcached Considerations
4.5.2. System Time
4.5.3. General Service Tuning
4.5.4. Filesystem Considerations
4.5.5. General System Tuning
4.5.6. Logging Considerations
4.5.7. Working with Rings
4.5.8. The Account Reaper
4.6. Replication
4.6.1. Database Replication
4.6.2. Object Replication
4.7. Managing Large Objects (Greater than 5 GB)
4.7.1. Using st to Manage Segmented Objects
4.7.2. Direct API Management of Large Objects
4.7.3. Additional Notes on Large Objects
4.7.4. Large Object Storage History and Background
4.8. Throttling Resources by Setting Rate Limits
4.8.1. Configuration for Rate Limiting
4.9. Managing OpenStack Object Storage with ST
4.9.1. ST Basics
4.9.2. Analyzing Log Files with ST

By understanding the concepts inherent to the Object Storage system you can better monitor and administer your storage solution.




4.1. Understanding How Object Storage Works

This section offers a brief overview of each concept in administering Object Storage.

Proxy Server

swift 아키텍쳐의 나머지와 함께 시도를 위한 응답이 가능하다.
각 요청을 위해 어카운트,컨테이너 또는 링의 오프젝트의 위치를 찾을것이고,
그에 부흥해서 요청을 라우트 할것이다.
public API 는 프록시 서버를 통해 또한 드러나게 된다.

오류의 많은 수들은 프록시 서버에서 또한 조정된다
예를 들면 만약 서버가 object PUT 때문에 사용이 불가하다면,
이는 handoff 서버를 위해 ring 을 요청할것이다.
그리고 대신 거기로 라우트할것이다.

object 들이 object 서버로 부터 또는 에서 스트림되면, 그들은 사용자로부터 또는 에서 프록시
서버를 통해 직접적으로 스트림된다. - 프록시 서버는 그들을 스풀하지 않는다.

The Ring
ring 은 그들의 물리적 위치와 디스크 상에 저장된 작격들의 이름 사이 매핑을 대신한다.
어카운트,컨테이너와 오브젝트을 위한 구분된 링들이 있다.
다른 컴포넌트들이 오프젝트,컨테이너 또는 어카운트상에서 어떤 오퍼레인션을 실행할
필요가 있을때, 그들은 클러스터내에 그것의 위치를 결정하기 위해 점유한 링들과 함께
연동할 필요가 있다.

링은 사용중인 존,디바이스,파티션,복제한 이 매핑을 유지보수한다. 링내에 각 파티션은
복제되고, 기본값에 의해 파티션을 위한 위치와 클러스터를 지나 3번은 링에 링에 의해서
유지된 매핑내에 저장된다. 링은 또한 디바이스가 실패 시나리오에서 handoff 를 위해
사용된것을 결정하기 위해서 응답가능하다.

데이터는 링에서 존의 컨셉으로 격리될수 있다. 파티션의 각 복제는 다른존에서 거주하는것
이 보장된다. 존은 드라이브,서버,캐비넷,스위치 또는 데이터센터 조차 대신할수 있다.

링의 파티션들은 swift 설치에서 전체 디바이스들 사이에서 동등하게 구분된다.
파티션이 주의를 움직이게 될 필요가 있을때(예를들면, 만약 디바이스가 클러스터에 추가되면)
,링은 파티션의 최소수가 그때에 이동되는것을 보장한다, 그리고,
오직 한 파티션의 복제가 그때에 이동된다.

가중치는 클러스터를 지나 드라이브상의 파티션의 배포를 밸런스하기 위해 사용될수 있다.
이는 유용할수 있고, 예를들면, 클러스터내에 다른 사이즈 드라이브들이 사용될때 유용할수
있다.

링은 프록시서버와 몇몇 백그라운드 프로세스들(복제와 같은)에 의해 사용된다.

Object Server
오브젝트 서버는 로컬 디바이스상에서 저장된 오브젝트들을 저장, 회수, 삭제할수 있는 아주 단순한 blob 스토리지 서버이다.
오브젝트는 파일의 확장 속성(xattrs)내에 저장된 메타데이터로 파일시스템상에 바이너리 파일들로 저장된다.
이는 오브젝트 서버를 위해 밑에 있는 파일시스템 선택은 파일들에서 xttrs 를 지원하는것을 요구한다.
ext3 같은 일부 파일시스템은 기본값에 의해서 xattrs 이 꺼져있다.

각 오브젝트는 오브젝트 이름의 해쉬와 오퍼레인션의 타임스탬프로 부터 파생된 경로를 사용해서 저장된다.
최근 쓰기는 항상 얻고, 최근 오브젝트 버전이 제공될것을 보장한다. 삭제는 또한 파일의 버전으로 다루어지게 된다.
( tombstone 을 상징하는 ".ts" 로 끝나는 0바이트 파일 ) 이것은 삭제된 파일이 정확히 복제되는것을 보장하고,
오래된 버전은 실패 시나리오 때문에 이상하게 다시 나타나지 않는다.


Container Server
컨테이너 서버의 첫번째 job 은 오브젝트들의 목록을 핸들링 하는것이다 이는 다는 오브젝트가 기술된 컨테이너내에
있는것, 그 오브젝트들이 어디에 있는지 모른다. 목록들은 sqlite 데이터베이스 파일들로 저장되고, 어떻게 오브젝트들이
있는것과 유사하게 클러스터를 지나 복제된다(?)
통계는 또한 오브젝트들의 전체수와 그 컨테이너를 위한 전체 스토리지 사용량를 포함한것을 추적(track)된다.

Account Server
어카운트 서버는 오브젝트 보다 컨테이너의 목록을 위해 응답가능한것을 제외하고 컨테이너 서버와 매우 유사하다.

Replication
복제는 드라이브 실패와 네트워크의 outages 와 같이 임시적인 오류 고려의 측면에서 지속상태내에 시스템을 유지하기
위해 디자인되었다.

복제 프로세스들은 최신버전을 포함한 모든것들을 보장하기 위해서 각 원격복사로 로컬 데이터를 비교한다.
오브젝트 복제는 공유된 상위  워터마트들과 해쉬의 조합을 사용한 각 파티션,컨테이너,어카운트 복제의
하위섹션을 빠르게 비교하기 위해서 해쉬목록을 사용한다.

복제 업데이트는 push 를 기반으로 한다 object 복제를 위해서는 업데이팅은 그냥 peer 로 파일들의 rsyncing 의 일이다.
어카운트와 컨테이너 복제 push missing 은 HTTP 를 거쳐 레코드하거나 전체 데이터베이스를 rsync 한다.

복제자 도한 데이터가 시스템으로부터 remove 되는것을 보장한다. item(오브젝트,컨테이너 또는 어카운트)가 삭제될때,
tombstone 은  item 의 최신버전으로 설정된다. 복제자는 tombstone 을 볼것이고, item 이 전체시스템으로부터 remove 되는것을
보장한다.


Updaters

컨테이너 또는 어카운트 데이터가 즉시 업데이트 될수 없을때, 시간들이 있다. 이는 주로 실패 시나리오 또는 고부하 기간동안에 발생한다.
업데이트가 실패한다면, 업데이트는 파일시스템상에 큐 되고, 업데이터는 실패된 업데이트들을 처리할것이다. 이는 최종적인 일관성 윈도우가 플레이를 위해
아마 들어올것이다. 예를들면, 컨테이너 서버가 저부하이고, 새 오브젝트가 시스템으로 들어가게 되는것을 추정해라.
오브젝트는 프록시 서버가 클라이언트로 성공으로 응답을 하자마자 읽기를 위해 즉시 가용될것이다. 그라나 컨테이너 서버는 오브젝트 리스팅을 업데이트 하지 않았고,
그래서 업데이트는 나중에 업데이트를 위해 큐 되어질것이다. 컨테이너 목록들, 그러므로 오브젝트는 즉시 포함하지 않을수 있다.

실제는, 일관성 윈도우는 업데이터가 실행하는 주기만큰 큰것이고, 프록시 서버가 어떤 응답하는 첫번째 컨테이너 서버로 목록 요청들을 라우트 하는것으로서 공지조차 되지
앓을수 있다. 서버의 저부하는 목록을 핸들할수도 있는 다른 두개의 복제의 subsequent 목록 요청 - 하나를 제공하는 한개가 되지 않을수 있다?


Auditors
Audiors 는 오브젝트,컨테이너,어카운트의 통합을 체크중인 로컬서버를 긴다? 만약 부패가 발견되면 ( 비트 부패(bit rot) 의 경우에, 예를 들면 ), 파일은 보장되어지고, 복제는 다른 복제로 부터 나쁜파일을 대체할것이다. 만약 다른 에러들이 발견이 되면 그들은 로그된다. ( 예를들면, 오브젝트의 리스팅은 이것이여야만 한느 어떠한 컨테이너 서버상에서 발견될수 없다 )



4.2. OpenStack 오브젝트 스토리지 설정과 튜닝

이 섹션은 배치 옵션과 고려사항을 통해서 walks 한다.

~에서 선택하기 위한 멀티 배치 옵션을 가진다. swift 서비스는  swift 를 위해 하드웨어 배치를 설계할때 많은 유연성을 위해 제공하는것을 
자체적으로 완전히 동작한다. 4개의 주요 서비스는


Proxy Services

Object Services

Container Services

Account Services

Proxy 서비스는 많은 CPU 와 네트워크 I/O 를 잡아먹는다. ( 짧은 시간이 많은 일을 한다 ). 
만약 proxy 로 10g 네트워킹을 사용중이거나, proxy 에 SSL 트래픽을 terminating 중이라면 더 많은 CPU 파워가
요구될것이다.

오브젝트, 컨테이너와 어카운트 서비스들 ( 스토리지 서비스 ) 은 많은 디스크와 네트워크 I/O 를 잡아먹는다.

가장 쉬운 배치는 각 서버에 모든 서비스를 설치하는것이다.  이는 각 서비스들을 수평으로 스케일 아웃하는것으로서 이를 하는것과 함께 나쁜것은 없다.

Rackspace 에서 우리는 그들 소유의 서버에서 Proxy 서버와 같은 서버에서 스토리지 서비스의 모든것을 넣는다.
이는 proxy 로 10g 네트워킹과 스토리지 서버들로 1g 를 보내는것을 우리에게 허가하고, 관리가능한 많은 proxy 들로 로드밸런싱을 유지한다.
스토리지 서비스는 스토리지 서버가 추가되는것으로서 수평적으로 스케일아웃 하고, 더 많은 Proxy 들을 더하므로서 전체 API 처리량을 확장할수 있다.

만약 어카운트 또는 컨테이너 서비스 둘 중 하나로 더 많은 처리량이 필요하다면 그들은 그들 소유의 서버로 각각 배치될수도 있다.
예를 들자면, 데이터베이스에서 더 빠른 디스크 I/O 를 얻기 위해 빠른 ( 하지만 더 많이 비싼 ) SAS 또는 SSD 드라이브 조차 사용할수도 있다.

로드밸런싱과 네트워크 설계는 reader 로 exercise 로서 left 이다. 하지만 이는 클러스터의 아주 중요한 부분이다.
그래서 시간은 Swift 클러스터를 위해서 네트워크 설계가 소비되어야만 한다.


4.3. Ring 준비하기

첫번째 단계는 링내에 있을 파티션의 수를 결정하는것이다. 우리는 드라이브를 건너서 배포본까지 보장하기 위해서 드라이브 당 최소 100개의 파티션이 있는것을 권장한다.
좋은 시작 포인트는 클러스터가 포함할 드라이버의 최대수를 해결하는것이 좋을수 있고, 그리고 나서 100 으로 멀티하게, 그리고 나서 두개의 가까운 파워로 올리는것이다.

예를 들면, 5000 드라이브 보다 더 많이 가지지 않을 클러스터를 빌딩하는것을 상상하라. 그것은 2^19 로 꽤 근접한 올리는 500,000 파티션의 전체수를 가지게 될것을 의미할것이다.

이는 또한 작은 ( relatively ) 파티션의 수를 유지하기 위한 좋은 아이디어 이다. 많은 파티션 있는, 다른 백엔드 job 과 replicators 에 의해 끝나게 되어 하는 더 많은 작업이 있고,
프로세스에서 링이 소비하는 더 많은 메모리가 있다. 목표는 최소 링들과 최대 클러스터 사이지사이에서 좋은 균형을 찾는것이다.

다음 단계는 데이터의 저정을 위한 replicas 의 수를 결정하는것이다. 현재 3 을 사용하는것을 권장한다. ( as this is the only value that has been tested )
더 높은 수는 사용된 더 많은 스토리지 하지만 당신이 데이터를 유실할듯한것보다 적게..

이는 또한 얼마나 많은 존들이 클러스터를 가져야하는지를 결정하기 위해 중요하다. 이는 최소 5개의 존으로 시작하는것을 권장한다.
또한 적게 시작할수 있다 하지만 우리의 테스팅은 실패가 발생할때 최소 다섯개의 존이 최상이라는것을 보여준다.
우리는 또한 가능한한 많은 격리로 생성하는것을 가능한한 높은 레벨에서 존들을 설정을 시도하는것을 권장한다.
고려사항에서 가지는 몇몇 예제 things 는 물리적인 위치, 파워 availability 와 네트워크 연결성을 포함할수 있다.
예를들면, 작은 클러스터 내에서 cabinet 에 의해 존들을 분리하는것을 결정할수도 있다. 그 소유릐 파워와 네트워크 연결성을 가지는 각 cabinet 과 함께..
존 개념은 아주 추상적이고, 그래서 실패에서 당신의 데이터를 분리하는 최선의 어떠한 방법이라도 이를 사용하는것은 feel free 하다.
존은 1로 시작하는 숫자로 참조되어진다.

당신은 현재 링을 빌딩하는것을 아래와 함께 시작할수 있다.

swift-ring-builder <builder_file> create <part_power> <replicas> <min_part_hours>

이는  2^<part_power> 파티션으로 <builder_file> 을 생성하는 링 빌드 프로세스를 시작할것이다. 

<min_part_hours> 는 specific 파티션이 succession 에서 삭제될수 있기 전에 시간(hours) 이다.
(24 is a good value for this).

디바이스들은 아래와 함께 링으로 더하게 될수 있다.:

swift-ring-builder <builder_file> add z<zone>-<ip>:<port>/<device_name>_<meta> <weight>

이는 링으로 디바이스를 추가할것이다. <builder_file> 은 이전에 생성되어진 builder 의 이름에서 링으로 디바이스를 추가할것이다.
<zone> 은 이 디바이내에 존의 숫자이다, <ip> 는 디바이스에 있는 서버의 IP 주소이다. <port> 는 서버가 실행중인 포트 숫자이다, <device_name> 은 서버상에서 디바이스의 이름이다. (예:sdb1)
<meta>는 디바이스를 위한 메타데이터의 문자열이다. (optional) 그리고, <weight> 는 얼마나 많은 파티션들이 클러스터에서 디바이스의 여분으로 관계된 디바이스으로 넣을 것인지를 결정하는
float weight 이다. ( 좋은 시작 포인트는 드라이브상에서 100.0 X TB 이다. ) 클러스터에서 최초로 있을 각 디바이스를 추가하라.

한번 모든 디바이스를 링으로 추가하게 되면, 구동하라:

swift-ring-builder <builder_file> rebalance

이것은 링내에 드라이브를 지나서 파티션을 배포할것이다. 이는 rebalance 구동전에 요구된 모든 변경을 만들기 위해서 링으로 어디에서든 변경을 만드는것으로 중요하다.??
이것은 가능한 균형된것으로서, 가능한한 적은 파티션이 이동되는것으로서 링이 머무는것을 보장할것이다.

위의 프로세스는 각 스토리지 서비스 ( 어카운트, 컨테이너 와 오브젝트 ) 를 위해 링을 만들기 위해 종료되어야 한다. builder 파일은 링으로 미래의 변경들에서 필요하게 될것이고,
그래서 이는 이들이 유지되어지고 백업되어지는것이 매우 중요하다. 결과치 .tar.gz 링 파일은 클러스터내에 모든 서버로 푸쉬되어져야 한다. building 링에 대한 더 많은 정보를 위해서는,
옵션없이 swift-ring-builder  구동하는것은 사용가능한 명령과 옵션으로 help 텍스트를 디스플레이 할것이다.


4.4. 서버 설정 레퍼런스

Swift 는 서버 설정을 관리하기 위해서 paste.deploy 를 사용한다. 기본 설정 옵션은 [DEFAULT] 섹션에서 설정하고,
다른 섹션의 어떤곳에서 오버라이딩될수 있는 기술된 어떤 다른 옵션들을 설정한다.


4.4.1. 오브젝트 서버 설정

예제 오브젝트 서버 설정은 소스코드 저장소내에 etc/object-server.conf-sample 에서 찾을수 있다.

따라오는 설정옵션은 사용가능하다.:


Table 4.1. object-server.conf Default 옵션은 [DEFAULT] 섹션에 있다.

옵션   	        디폴트	      설명
swift_dir	        /etc/swift	      Swift 설정 디렉토리
devices	        /srv/node	      디바이스가 마운트될 부모 디렉토리
mount_check	true	              root 디바이스로 뜻하지 않는 쓰기를 예방하기 위해 디바이스를 마운트할지 하지 않을지를 체크
bind_ip	        0.0.0.0	      바인딩하는 서버의 IP 주소
bind_port	        6000	              바인딩하는 서버의 포트
workers	        1	              fork 할 worker 의 수


Table 4.2. [object-server] 섹션내에 object-server.conf  서버 옵션

옵션	                        디폴트	        설명
use		                paste.deploy      오브젝트 서버를 위한 엔트리 포인트. 대부분의 경우를 위해 이는 egg:swift#object 이어야 한다.
log_name	                object-server	로깅할때 사용된 레이블
log_facility	                LOG_LOCAL0	Syslog log facility
log_level	                INFO	                로그 레벨
log_requests	        True	                각요청을 로그로 남길지 않을지 설정
user	                        swift	                구동할 사용자
node_timeout	        3	                외부 서비스로 요청 타임아웃
conn_timeout	        0.5	                외부 서비스로 연결 타임아웃
network_chunk_size	65536	        네트워크를 거쳐 read/write 를 위한 chunk 크기
disk_chunk_size	        65536	        디스크로 read/writre 를 위한 chunk 크기
max_upload_time	86400	        오브젝트를 업로드 하기 위한 허가된 최대 시간
slow	                        0	                0보다 크면, 완료하는 PUT 또는 DELETE 요청을 위한 최소 시간 ( 초 )


Table 4.3. [object-replicator] 섹션내에 object-server.conf Replicator 옵션

옵션	                 디폴트	             설명
log_name	         object-replicator      로깅할때 사용된 레이블
log_facility	         LOG_LOCAL0	     Syslog log facility
log_level	         INFO	                     로그레벨
daemonize	         yes	                     데몬으로 리플리케이션을 수행할지 않을지 설정
run_pause	         30	                     리플리케이션 패스 사이에서 기다릴 시간 (초)
concurrency	 1	                     spawn 할 리플리케이션 woker 의 수
timeout	         5	                     Timeout value sent to rsync ?timeout and ?contimeout options
stats_interval	 3600	             로깅 리플리케이션 통계 사이의 간격(초)
reclaim_age	 604800	             오브젝트가  되찾게 될수 있기 전 경과시간(초)


Table 4.4. [object-updater] 섹션내에 object-server.conf Updater 옵션

옵션	                 디폴트	             설명
log_name	         object-updater	      로깅될때 사용된 레이블
log_facility	         LOG_LOCAL0	      Syslog log facility
log_level	         INFO                      로그레벨
interval	         300	                      가져오는 패스를 위한 최소 시간
concurrency	 1	                      spawn 할 업데이터 워커의 수
node_timeout	 10	                      외부 서비스로 요청 타임아웃
conn_timeout	 0.5	                      외부 서비스로 연결 타임아웃
slowdown	         0.01	                      오브젝트 사이 기다릴 시간 (초)


Table 4.5.  [object-auditor] 섹션내에 object-server.conf Auditor 옵션

옵션                      디폴트	              설명
log_name	            object-auditor	      로깅될때 사용된 레이블
log_facility	            LOG_LOCAL0	      Syslog log facility
log_level	            INFO	              로그레벨
files_per_second	    20	                      초당 감사된 최대 파일. 각각 시스템 스펙에 따라 켜져야 한다. 0 은 제한없음.
bytes_per_second  10000000	      초당 감사된 최대 바이트. 각각 시스템 스펙에 따라 켜져야 한다. 0은 제한없음.


4.4.2. 컨테이너 서버 설정

예제 컨테이너 서버 설정은 소스코드 저장소내에 etc/container-server.conf-sample 에서 찾을수 있다.

아래의 설정옵션은 사용가능하다.:


Table 4.6. [DEFAULT] 섹션내에 container-server.conf 디폴트 옵션

옵션	                 디폴트           설명
swift_dir	        /etc/swift       Swift 설정 디렉토리
devices	        /srv/node	     디바이스가 마운트될 부모 디렉토리
mount_check	true	             root 디바이스로 뜻하지 않은 쓰기를 방지하기 위해 디바이스를 마운트 할지 하지 않을지를 체크
bind_ip	        0.0.0.0	     바인딩할 서버 IP 주소
bind_port	        6001	             바인딩할 서버 포트
workers	        1	             fork 할 worker 의 수
user	                swift	             실행할 사용자


Table 4.7. [container-server] section 내에 container-server.conf 서버 옵션

옵션	                디폴트	         설명
use		        paste.deploy        entry point for the container server. For most cases, this should be egg:swift#container.
log_name	        container-server	  Label used when logging
log_facility	        LOG_LOCAL0	  Syslog log facility
log_level	        INFO	                  Logging level
node_timeout	3	                  Request timeout to external services
conn_timeout	0.5	                  Connection timeout to external services


Table 4.8.  [container-replicator] 섹션내에 container-server.conf Replicator 옵션

옵션                   디폴트	                  설명
log_name	         container-replicator	  로깅할때 사용된 레이블
log_facility	         LOG_LOCAL0	          Syslog log facility
log_level	         INFO	                          로그레벨
per_diff	         1000	
concurrency	 8	                          spawn 할 리플리케이션 worker 의 수
run_pause	         30	                          리플리케이션 패스들 사이에 기다릴 시간 (초)
node_timeout	 10	                          외부 서비스로 요청 타임아웃
conn_timeout	 0.5	                          외부 서비스로 연결 타임아웃
reclaim_age	 604800	                  컨테이너가 되찰을수 있기 전 경과된 시간 (초)


Table 4.9. [container-updater] 섹션내에 container-server.conf Updater 옵션

옵션	                  디폴트	                  설명
log_name	          container-updater	  로깅할때 사용된 레이블
log_facility	          LOG_LOCAL0	          Syslog log facility
log_level	          INFO	                  로그레벨
interval	          300	                          가져온 패스를 위한 최소 시간
concurrency	  4	                          spawn 할 updater worker 의 수
node_timeout	  3	                          외부 서비스로 요청 타임아웃
conn_timeout	  0.5	                          외부 서비스로 연결 타임아웃
slowdown	          0.01	                  컨테이너 사이에 기다릴 시간(초)


Table 4.10. [container-auditor] 섹션내에 container-server.conf Auditor 옵션

옵션	             디폴트                         설명
log_name	     container-auditor	        로깅할때 사용된 레이블
log_facility	     LOG_LOCAL0	                Syslog log facility
log_level	     INFO	                        로그레벨
interval	     1800	                        가져올 패스를 위한 최소 시간


4.4.3. 어카운트 서버 설정

예제 어카운트 서버 설정은 소스코드 저장소내에 etc/account-server.conf-sample 에서 찾을수 있다.

따라오는 설정옵션은 사용가능하다.


Table 4.11.  [DEFAULT] 섹션내에 account-server.conf 디폴트 옵션

옵션	                디폴트	     설명
swift_dir	        /etc/swift	     Swift 설정 디렉토리
devices	        /srv/node	     디바이스를 마운트할 또는 부모 디렉토리
mount_check	true	             root 디바이스로 뜻하지 앟는 쓰기를 방지하기 위해 디바이스를 마운트할지 하지 않을지 체크
bind_ip	        0.0.0.0	     바인딩할 서버 IP 주소
bind_port	        6002	             바인딩할 서버 포트
workers	        1	             fork 할 worker 의 수
user	                swift	             실행할 사용자


Table 4.12.  [account-server] 섹션내에 account-server.conf 서버 옵션

옵션	                디폴트	        설명
use		        어카운트 서버를 위해 paste.deploy 를 위한 엔트리 포인트. 대부분 경우를 위해서 이는 egg:swift#account 가 되어야 한다.
log_name	        account-server	로깅할때 사용된 레이블
log_facility	        LOG_LOCAL0	Syslog log facility
log_level	        INFO 	                로그레벨


Table 4.13.  [account-replicator] 섹션내에 account-server.conf Replicator 옵션

옵션	                 디폴트	                 설명
log_name	         account-replicator       로깅할때 사용된 레이블
log_facility	         LOG_LOCAL0	         Syslog log facility
log_level	         INFO                         로그레벨
per_diff	         1000	
concurrency	 8	                         spwan 할 리플리케이션 worker 의 수
run_pause	         30	                         리플리케이션 패스 사이에 기다릴 시간 (초)
node_timeout	 10	                         외부 서비스로 요청 타임아웃
conn_timeout	 0.5	                         외부 서비스로 접속 타임아웃
reclaim_age	  604800	                 어카운트가 되찾게 될수 있기 전에 경과된 시간 (초)


Table 4.14.  [account-auditor] 섹션내에 account-server.conf Auditor 옵션

옵션                   디폴트                        설명
log_name	         account-auditor	          로깅할때 사용된 레이블
log_facility	         LOG_LOCAL0	           Syslog log facility
log_level	         INFO                           로그레벨
interval	         1800	                   가져올 패스를 위한 최소 시간


Table 4.15. [account-reaper] 섹션내에 account-server.conf Reaper 옵션

옵션                   디폴트                         설명
log_name	         account-auditor	            로깅될때 사용된 레이블
log_facility	         LOG_LOCAL0	            Syslog log facility
log_level	         INFO                            로그레벨
concurrency	 25	                            spawn 할 리플리케이션 worker 의 수
interval	         3600	                    가져올 패스를 위한 최소 시간
node_timeout	 10	                            외부 서비스를 위한 요청 타임아웃
conn_timeout	 0.5	                            외부 서비스로 연결 타임아웃


4.4.4. Proxy 서버 설정

예제 Proxy 서버 설정은 소스코드 저장소내에 etc/account-server.conf-sample 에서 찾을수 있다.

따라오는 설정옵션은 사용가능하다.


Table 4.16.  [DEFAULT] 섹션내에 proxy-server.conf 디폴트 옵션

옵션	          디폴트              설명
bind_ip	  0.0.0.0	         바인딩할 서버 IP 주소
bind_port	  80	                 바인딩할 서버 포트
swift_dir	  /etc/swift	         Swift 설정 디렉토리
workers	  1	                 fork 할 worker 의 수
user	          swift	         실행할 사용자
cert_file		                 ssl .crt 의 경로
key_file		                 ssl .key 의 경로


Table 4.17.  [proxy-server] 섹션내에 proxy-server.conf 서버 옵션

옵션                                          디폴트                   설명
use		                                Proxy 서버를 위해 paste.deploy 를 위한 엔트리 포인트. 대부분의 경우를 위해 이는 egg:swift#proxy 이 되어야 한다.
log_name	                                proxy-server	    로깅할때 사용된 레이블
log_facility	                                LOG_LOCAL0	    Syslog log facility
log_level	                                INFO	                    로그레벨
log_headers	                        True	                    True 이면 각 요청에서 헤더들에 로그
recheck_account_existence	        60	                    어카운트 존재를 위해 memcached 를 보내는 Cache 타임아웃(초)
recheck_container_existence	60	                    컨테이너 존재를 위해 memcached 를 보내는 Cache 타임아웃(초)
object_chunk_size	                65536	            오브젝트 서버로 부터 읽은 Chunk 사이즈
client_chunk_size	                        65536	            클라이언트로 부터 읽은 Chunk 사이즈
memcache_servers	                127.0.0.1:11211	    memcached 서버 IP:PORT 의 콤마로 구분된 목록
node_timeout	                        10	                    외부 서비스로 요청 타임아웃
client_timeout	                        60	                    클라이언트에서 한 chunk 를 읽은 타임아웃
conn_timeout	                        0.5	                    외부 서비스로 연결 타임아웃
error_suppression_interval	        60	                    더이상 제한된 에러를 간주되지 않은 노드를 위한 마지막 에러이래 경과해야 하는 시간(초)
error_suppression_limit	                10	                    제한된 노드 에러를 간주하는 에러 카운트
allow_account_management 	false	                    Whether account PUTs and DELETEs are even callable


Table 4.18. [filter:swauth] 섹션내에 proxy-server.conf Paste.deploy 옵션

옵션                          디폴트                                      설명
use		                auth 를 위해 사용한 past.depoly 를 위한 엔트리 포인트, swauth 포함 해서 사용을 위해 egg:swift#swauth 로 설정한다.
log_name	                auth-server	Label                         로깅할때 사용됨
log_facility	                LOG_LOCAL0	                        Syslog log facility
log_level	                INFO	                                        로그레벨
log_headers	        True	                                        True 면, 각 요청에 헤더들 로그
reseller_prefix	        AUTH	                                auth 서비스를 위한 네이밍 scope. Swift 스토리지 어카운트와 auth 토큰들은 이 prefix 로 시작할것이다.
auth_prefix	                /auth/	                                auth 서비스를 위한 HTTP 요청 경로 prefix. Swift 자체는 letter v 와 함께 시작하는 어떠한것들을 예약한다.
default_swift_cluster	local#http://127.0.0.1:8080/v1	새롭게 생성된 어카운트로 대체를 위한 디폴트 Swift 클러스터; 인증을 위한 Swauth 를 사용중이라면 오직 필요하다.
token_life	                86400	                                토큰이 유효한 시간 (초)
node_timeout	        10	                                         요청 타임아웃
super_admin_key	         None	                                 .super_admin 어카운트를 위한 키



4.5. Considerations and Tuning

Fine-tuning your deployment and installation may take some time and effort. Here are some considerations for improving performance of an OpenStack Object Storage installation.

4.5.1. Memcached Considerations

Several of the Services rely on Memcached for caching certain types of lookups, such as auth tokens, and container/account existence. Swift does not do any caching of actual object data.
Memcached should be able to run on any servers that have available RAM and CPU. At Rackspace, we run Memcached on the proxy servers. 
The memcache_servers config option in the proxy-server.conf should contain all memcached servers.


4.5.2. System Time

Time may be relative but it is relatively important for Swift! Swift uses timestamps to determine which is the most recent version of an object. 
It is very important for the system time on each server in the cluster to by synced as closely as possible (more so for the proxy server, but in general it is a good idea for all the servers). 
At Rackspace, we use NTP with a local NTP server to ensure that the system times are as close as possible. This should also be monitored to ensure that the times do not vary too much.


4.5.3. General Service Tuning

Most services support either a worker or concurrency value in the settings. This allows the services to make effective use of the cores available. 
A good starting point to set the concurrency level for the proxy and storage services to 2 times the number of cores available. If more than one service is sharing a server,
then some experimentation may be needed to find the best balance.

At Rackspace, our Proxy servers have dual quad core processors, giving us 8 cores. Our testing has shown 16 workers to be a pretty good balance when saturating a 10g network and 
gives good CPU utilization.

Our Storage servers all run together on the same servers. These servers have dual quad core processors, for 8 cores total. We run the Account, Container, 
and Object servers with 8 workers each. Most of the background jobs are run at a concurrency of 1, with the exception of the replicators which are run at a concurrency of 2.

The above configuration setting should be taken as suggestions and testing of configuration settings should be done to ensure best utilization of CPU, network connectivity, and disk I/O.



4.5.4. Filesystem Considerations

Swift is designed to be mostly filesystem agnostic?the only requirement being that the filesystem supports extended attributes (xattrs). After thorough testing with our use cases 
and hardware configurations, XFS was the best all-around choice. If you decide to use a filesystem other than XFS, we highly recommend thorough testing.

If you are using XFS, some settings that can dramatically impact performance. We recommend the following when creating the XFS partition:

mkfs.xfs -i size=1024 -f /dev/sda1

Setting the inode size is important, as XFS stores xattr data in the inode. If the metadata is too large to fit in the inode, a new extent is created, which can cause quite a performance problem.
Upping the inode size to 1024 bytes provides enough room to write the default metadata, plus a little headroom. We do not recommend running Swift on RAID, 
but if you are using RAID it is also important to make sure that the proper sunit and swidth settings get set so that XFS can make most efficient use of the RAID array.

We also recommend the following example mount options when using XFS:

mount -t xfs -o noatime,nodiratime,nobarrier,logbufs=8 /dev/sda1 /srv/node/sda

For a standard swift install, all data drives are mounted directly under /srv/node (as can be seen in the above example of mounting /dev/sda1 as /srv/node/sda). 
If you choose to mount the drives in another directory, be sure to set the devices config option in all of the server configs to point to 



4.5.5. General System Tuning

Rackspace currently runs Swift on Ubuntu Server 10.04, and the following changes have been found to be useful for our use cases.

The following settings should be in /etc/sysctl.conf:


        
# disable TIME_WAIT.. wait..
net.ipv4.tcp_tw_recycle=1
net.ipv4.tcp_tw_reuse=1

# disable syn cookies
net.ipv4.tcp_syncookies = 0

# double amount of allowed conntrack
net.ipv4.netfilter.ip_conntrack_max = 262144

    

To load the updated sysctl settings, run sudo sysctl -p

A note about changing the TIME_WAIT values. By default the OS will hold a port open for 60 seconds to ensure that any remaining packets can be received. During high usage,
and with the number of connections that are created, it is easy to run out of ports. We can change this since we are in control of the network. If you are not in control of the network,
or do not expect high loads, then you may not want to adjust those values.


4.5.6. Logging Considerations

Swift is set up to log directly to syslog. Every service can be configured with the log_facility option to set the syslog log facility destination.
We recommend using syslog-ng to route the logs to specific log files locally on the server and also to remote log collecting servers.



4.5.7. Rings 과 함께 작동하기

링은 클러스터에서 데이터가 어디에 거주해야 하는지 결정한다. 어카운트 데이터베이스, 컨테이너 데이터베이스와 개개의 오브젝트를 위한 분리된 링이 있다. 하지만 각 링은 같은 방법내에서 작동한다.
이 링들은 외부적으로 관리되고, 그 서버 프로세스 그들 자체에서 링을 수정하지 않는다, 그들은 다른 도구에 의해서 주어진 수정된 새 링들을 대신한다.

링은 디바이스를 저장한 파티션 인덱스로 경로의 MD5 해쉬로 부터 설정가능한 비트수를 사용한다. 비트의 수는 파티션 파워로 알려진 해쉬로 부터 유지된다.
그리고, 파티션 파워의 2 는 파티션 카운트를 나타낸다. 전체 MD5 해쉬 링을 파티셔닝하는것은 즉시 전체 모든 클러스터 모두 또는 제각기 각 아이템과 함께 작동하는것 보다 최소한 덜 복잡한것중 
더 효율적인것 하나를 끝나는 즉시 아이템의 배치들에서 작동하기 위한 클러스터의 다른 부분들을 허가한다.

다른 설정가능한 값들은 얼마나 많은 파티션-디바이스 할당이 단일 링을 차지하는가를 가리키는 replica 카운트 이다.
주어진 파티션 수를 위해, 각 replica 의 디바이스는 다른 replica 의 디바이스로 같은 존내에 존재하지 않을것이다. 존들은 물리적 위치, 파워 분리, 네트워크 분리 또는 동시에 사용가능하지 않은
멀티 replicas 를 줄일 어떤 다른 속성을 기반으로 한 그룹 디바이스로 사용될수 있다.


4.5.7.1. Ring Builder 와 함께 Ring 들 관리하기

링은 ring-builder 로 불리는 유틸리티에 의해 수동으로 관리하고 빌드되어진다. ring-builder 는 디바이스로 파티션을 할당하고, 서버로 보내기 위해 디스크 상에서 gzipped, pickled 파일로
최적화된 파이선 구조를 작성한다.
서버 프로세스는 단지 가끔 파일의 수정시간을 확인하고, 필요됨에 따라서 링 구조의 그들의 in-memory 복사를 릴로드 한다.
왜냐하면 ring-builder 가 어떻게 링으로 변경들을 관리하는지, 조금 오래된 링을 사용하는것은 단지.. usually 쉽게 돌리게 될수 있는 ( 피하게 될수 있는 ) 것.. 올바르지 않게 될 파티션의 subset
을 위한 세개의 replicas 중 하나를 의미한다...

The rings are built and managed manually by a utility called the ring-builder. The ring-builder assigns partitions to devices and writes an optimized Python structure to a gzipped,
pickled file on disk for shipping out to the servers. The server processes just check the modification time of the file occasionally and reload their in-memory copies of the ring structure as needed.
Because of how the ring-builder manages changes to the ring, using a slightly older ring usually just means one of the three replicas for a subset of the partitions will be incorrect, 
which can be easily worked around.

ring-builder 는 또한 링 정보와 향후 링들을 빌드하기 위해 요구된 부가적인 데이터로 그 소유의 builder 파일을 유지한다.
이는 이 builder 파일의 멀티 백업 복사본들을 유지하기 위해 매우 중요하다.
한 옵션은 그들 스스로 링 파일들을 복사하는중에 매 서버로 builder 파일을 copy out 하는것이다. 다른 하나는 클러스터 자체로 builder 파일을 업로드 하는것이다.
builder 파일의 완전한 유실은 스크래치로 부터 새 링을 생성하는것을 의미할것이고, 거의 모든 파티션은 다른 디바이스로 할당되어서 끝날것이다. ( 처하게 될것이다. )
그 때문에 ( and therefore ) 저장된 모든 데이터는 새 위치로 리플리케이션되어야만 할것이다. 그래서 builder 파일 유실에서 복구는 가능하다. 하지만 데이터는 확장된 시간을 위해 명백히
도착되지 않을것이다.

The ring-builder also keeps its own builder file with the ring information and additional data required to build future rings. It is very important to keep multiple backup copies of these builder files.
One option is to copy the builder files out to every server while copying the ring files themselves. Another is to upload the builder files into the cluster itself. 
Complete loss of a builder file will mean creating a new ring from scratch, nearly all partitions will end up assigned to different devices, 
and therefore nearly all data stored will have to be replicated to new locations. So, recovery from a builder file loss is possible, but data will definitely be unreachable for an extended time.

링 데이터 구조에 대해

링 데이터 구조는 세가지 최상위 레벨 필드로 구성된다: 클러스터내에 디바이스의 목록, 디바이스 할당으로 파티션을 지시하는 디바이스 ids 의 목록들의 목록,
그리고 해쉬를 위해 파티션을 계산하기 위해 MD5 hash 를 shift 하는 나타내는 비트수 ( 정수 )

The ring data structure consists of three top level fields: a list of devices in the cluster, a list of lists of device ids indicating partition to device assignments, 
and an integer indicating the number of bits to shift an MD5 hash to calculate the partition for the hash.


링내에 디바이스의 목록

디바이스의 목록은 devs 처럼 링 클래스로 내부적으로 얄려진다. 디바이스의 목록내에 각 아이템은 아래의 키와 함께 사전이다.:

Table 4.19. 디바이스와 키의 목록

키	         타입            설명 
id	         integer	   디바이스 목록내에 인덱스
zone	         integer	   디바이스가 거주(존재)하는 존
weight	 float	           다른 디바이스로 비교에서 디바이스의 상대적인 가중치. 이는 대게 디바이스가 다른 디바이스로 비교되어진 디스크 공간의 량으로 직접적으로 일치한다(들어맞다, 부합하다)
                                   예를 들어 공간의 1테라 바이트의 디바이스는 100.0 의 가중치를 가질수 있고, 2 테라 바이트 공간의 다른 디바이스는 200 의 가중치를 가질수 있다.
                                   이 가중치는 또한 바랬던 지간을 지나 적은 데이터 또는 많은 데이터와 함게 ended up 되어진 밸런스 디바이스에 도로 데려오기(bring back) 위해 사용되어 질수 있다.
                                   좋은 평균 가중치 100은 만약 필요하다면 나중에 가중치 저하에서 유연성을 허가한다.
ip	         string	   디바이스를 포함하는 서버 IP 주소
port	         int	           디바이스를 위해 요청을 제공하는 리스닝 서버 프로세스에서 사용하는 TCP 포트
device	 string	   서버상에서 디바이스의 디스크 이름. 예: sdb1
meta	         string	   디바이스를 위해 부가적인 정보를 저장하기 위해 일반적으로 사용하는 필드
                                   이 정보는 서버 프로세스에 의해 직접적으로 사용되지 않는다. 하지만, 디버깅에서 유용할수 있다.
				   예를들면, 데이터, 설치시간과 하드웨어 manufacturer 는 여기에 저장될수 있다.

Note:  디바이스의 목록은 클러스터에서 제거 되어진 디바이스를 위해서 None 으로 설정된 인덱스들이나, hole 들을 포함할수도 있다.
          일반적으로 디바이스 ids 는 재 사용되지 않는다. 또한 일부 디바이스들은 그들의 가중치를 0.0 으로 설정하므로서 임시적으로 비활성화 될수도 있다.


파티션 할당 목록

이는 디바이스 ids 의 배열(‘I') 의 목록이다. 가장 바깥쪽의 목록은 각 replica 를 위해 배열(‘I') 을 포함한다. 각 배열(‘I') 는 링을 위해 파티션 카운트로 동등한 길이를 가진다.
배열(‘I') 내에 각 정수는 위의 디바이스 목록에서 인덱스이다. 파티션 목록은 내부적으로 링 클래스 _replica2part2dev_id 로 알려져 있다.

그래서, 파티션으로 할당된 딕셔너리 디바이스의 목록을 생성하기 위해서, 파이선 코드 devices = [self.devs[part2dev_id[partition]] for part2dev_id in self._replica2part2dev_id] 처럼 보여질것이다.

배열(‘I')는  백만 파티션이 될수 있는것처럼 메모리 보존(보호)을 위해 사용되어 진다. 



파티션 Shift 값

파티션 shift 값은 내부적으로 링 클래스 _part_shift 로 알려져있다. 이 값은  해쉬가 존재해야 한느 그것을 위한 데이터상에 파티션을 계산하기 위해서 MD5 해쉬를 이동하기 위해 사용된다.
해쉬의 최상위 4 바이트는 오직 이 프로세스에서 사용되어진다. 예를 들면, 경로 /account/container/object 를 위한 파티션을 계산하기 위해서 
파이선 코드는 : partition = unpack_from('>I', md5('/account/container/object').digest())[0] >>self._part_shift 와 같이 나타낼수 있다.


4.5.7.2. Ring 빌드하기

링의 최초 빌드는  첫번째로 디바이스의 가중치를 기반으로 각 디바이스로 이상적으로 할당되어져야 하는 파티션의 수를 계산하는것이다.
예를 들면, 만약 20 의 파티션 파워.. 링은 1,048,576 파티션을 가질것이다. 만약 동한 가중치 1,000 디바이스가 있다면 그들은 각 1,048,576 파티션을 바랄것이다.
디바이스들은 그러면 그들이 바란 파티션의 수로 정렬되고, 프로세스 초기화 동안(내내) 적합하게 유지할것이다. 

그리고 나서, 링 builder 는 그 시점에서 대부분의 파티션들을 요구(바란)한 디바이스로 각 파티션의 replica 를 할당하고,
그 파티션을 위해 디바이스는 어떤 다른 replica 로 같은 존내에 있지 않은 제한과 함께...

한번(즉시) 할당되면, 디바이스의 요구(바란) 파티션 수는 감소되고, 디바이스의 목록에 그 소유의 새 정렬된 위치로 이동된고, 프로세스는 계속한다.

오래된 링을 기반으로 새 링을 빌딩할때, 각 디바이스가 원하는 바랜(desired) 파티션의 수는 다시 계산되어 진다.
다음 다시 할당되어진 파티션은 종합한다. ( 주워모으고, 따로따로 취급한것을 ). 그들의 할당된 파티션 모두를 가지는 어떤 삭제된 디바이스들은
할당하지 않고??? 종합된 목록으로 더한다.
그들이 지금 바라는것 보다 더 많은 파티션을 가지는 어떤 디바이스는 그들로 부터 할당되지 않은 랜덤 파티션들을 가지고, 종합된 리스트로 더하게 된다.
마지막으로, 종합된 파티션은 그러면 위에서 기술된 최초 할당에서 처럼 유사한 방법을 사용해서 디바이스로 재 할당된다.


파티션이 재할당된 replica 를 가질때마다, 재할당 시간은 기록된다.
이는 재 할다을 위해 파티션을 종합(gathering) 할때 어카운트로 가져간다??. (taken).... 설정가능한 시간량(amount of time ) 에서 두번 이동하는 파티션이 없도록
이 설정가능한 시간량(amount of time) 은 내부적으로 RingBuilder 클래서 min_part_hours 로 내부적으로 알려져 있다.
이 제한은 삭제되어진 디바이스상에서 파티션의 replicas 를 위해 무시되어진다.  디바이스를 삭제하는것이 오직 디바이스 실패로 일어나고, 선택의 여지가 없을때 하지만 재할당을 만들기 위해..

위의 프로세스는  재할당을 위해 파티션을 종합(gathering) 의 랜담 nature 때문에 항상 완벽히 ring 을 rebalnace 하지 않는다.
The above processes don't always perfectly rebalance a ring due to the random nature of gathering partitions for reassignment. 

더 많은 balaced 링을 reach 를 도우는것은 rebalance 프로세스를 거의 완벽히 할때까지 ( less 1% off ) 반복하거나, balance 가 최소 1% 에 로 향상하지 않을때
(  우리가 아마도 완벽한 balance 를 얻지 못할수 있는것을 나타내는것은 걷잡을수 없이 불균형된 존이나 최근 이동된 너무 많은 파티션들 때문이다. )


4.5.7.3. History of the Ring Design
링코드는 많은 반복을 거친다 ( go through ).. 이것이 어떻게 지금 도착하고 지금 잠시동안 안정되어지는 동안..??
알고리즘은 만약 새 아이디어들이 나오면 비들게 되거나, 아마도 근본적으로 까지 변경될수도 있다.
이 섹션은 시도된 이전 아이디어를 기술하는것을 시도할것이고 왜 그들이 버린지를 설명하는것을 시도할것이다.

A “live ring” option was considered where each server could maintain its own copy of the ring and the servers would use a gossip protocol to communicate the changes they made.
This was discarded as too complex and error prone to code correctly in the project time span available.
One bug could easily gossip bad data out to the entire cluster and be difficult to recover from. 
Having an externally managed ring simplifies the process, allows full validation of data before it's shipped out to the servers, and guarantees each server is using a ring from the same timeline.
It also means that the servers themselves aren't spending a lot of resources maintaining rings.

A couple of “ring server” options were considered. One was where all ring lookups would be done by calling a service on a separate server or set of servers,
but this was discarded due to the latency involved. Another was much like the current process but where servers could submit change requests to the ring server to have a new ring built 
and shipped back out to the servers. This was discarded due to project time constraints and because ring changes are currently infrequent enough that manual control was sufficient.
However, lack of quick automatic ring changes did mean that other parts of the system had to be coded to handle devices being unavailable for a period of hours until someone could manually update the ring.

The current ring process has each replica of a partition independently assigned to a device. A version of the ring that used a third of the memory was tried,
where the first replica of a partition was directly assigned and the other two were determined by “walking” the ring until finding additional devices in other zones. 
This was discarded as control was lost as to how many replicas for a given partition moved at once. 
Keeping each replica independent allows for moving only one partition replica within a given time window (except due to device failures). 
Using the additional memory was deemed a good tradeoff for moving data around the cluster much less often.

Another ring design was tried where the partition to device assignments weren't stored in a big list in memory but instead each device was assigned a set of hashes, or anchors. 
The partition would be determined from the data item's hash and the nearest device anchors would determine where the replicas should be stored. 
However, to get reasonable distribution of data each device had to have a lot of anchors and walking through those anchors to find replicas started to add up. 
In the end, the memory savings wasn't that great and more processing power was used, so the idea was discarded.

A completely non-partitioned ring was also tried but discarded as the partitioning helps many other parts of the system, especially replication. 
Replication can be attempted and retried in a partition batch with the other replicas rather than each data item independently attempted and retried. 
Hashes of directory structures can be calculated and compared with other replicas to reduce directory walking and network traffic.

Partitioning and independently assigning partition replicas also allowed for the best balanced cluster. 
The best of the other strategies tended to give +-10% variance on device balance with devices of equal weight and +-15% with devices of varying weights. 
The current strategy allows us to get +-3% and +-8% respectively.

Various hashing algorithms were tried. SHA offers better security, but the ring doesn't need to be cryptographically secure and SHA is slower. 
Murmur was much faster, but MD5 was built-in and hash computation is a small percentage of the overall request handling time. 
In all, once it was decided the servers wouldn't be maintaining the rings themselves anyway and only doing hash lookups, MD5 was chosen for its general availability, good distribution, 
and adequate speed.



4.5.8. 어카운트 Reaper

어카운트 Reaper 는 백그라운드에서 삭제된 계정에서 데이터를 제거한다.

어카운트는 서버의 remove_storage_account XMLRPC call 서비스를 통해서 reseller 에 의해 삭제를 위해 표시된다. 
An account is marked for deletion by a reseller through the services server's remove_storage_account XMLRPC call. 
이는 단순하게 어카운트 데이터베이스내에 ( 그리고 replicas ) account_stat table 의 status 컬럼으로 DELETED 값을 넣는것이고, 어카운트를 위해 데이터를 나타내는것은 나중에 삭제되어야 한다.
보유 시간 설정과 삭제되지 않은것은 없다; 이것은 reseller 가 각 기능을 구현할것을 추정되고, and only call remove_storage_account once it is truly desired the account's data be removed.

어카운트 reaper 는 각 어카운트 서버에서 동작하고, 삭제를 위해 표시된 어카운트 데이터베이스를 위해 가끔 서버를 스캔한다.
이것은 오직 서버는 첫번째 노드인 어카운트에서 트리거 할것이고, 그래서 멀티 어카운트 서버는 같은 시간에 같은 일을 하기 위해서 시도하는 모든것이 아닌것이다. ??
한 어카운트를 삭제하기 위해 멀티서버를 사용하는것은 스피드 삭제를 향상할수도 있다. 하지만 합동(조화)를 요구하고 그래서 그들은 노력을 중복중이 아니다.
속도는 정말 데이터 삭제와 함께 가급적 걱정거리가 아니다. 큰 어카운트들은 그것을 종종 삭제되지 않는다??
Speed really isn't as much of cercon with data deletion and large accounts are'nt deleted that often
속도는 정말 데이터 삭제와 함께 거의 걱정거리가 아니고 큰 어카운트들은 그것을 종종 삭제되지 않는다??

한 어카운트 자체를 위한 삭제 프로세서는 꽤  간단하다. 어카운트 내에 각 컨테이너를 위해, 각 오브젝트는 삭제되고 그리고 나서 컨테이너는 삭제된다.
실패한 어떤 삭제 요청은 전체 프로세스를 종료하지 않을것이지만, 결국 실패하기 위한 전체 프로세스의 원인일수 있다. ( 예를들면, 만약 오브젝트 삭제 타임아웃이면,
컨테이너는 나중에 삭제되는것이 가능하지 않을수 있다. 그 때문에 게다가 어카운트는 삭제되지 않을것이다. ) 

전체 프로세스는 실패시 조차 계속한다. 클러스터 공간을 되찾는것을 전전긍긍하지 않을수록 한 골치거리 spot 때문에 ???
The overall process continues even on a failure so that it doesn't get hung up reclaiming cluster space because of one troublesome spot. 

어카운트 reaper 는 결국 그것이 비워지게 될때까지 어카운트를 삭제 시도를 유지할것이다, 
데이터베이스는 db_replicator 와 함께 프로세스를 되찾는 시점에서 결국 데이터베이스를 삭제할것이다.
The account reaper will keep trying to delete an account until it eventually becomes empty, 
at which point the database reclaim process within the db_replicator will eventually remove the database files.


4.5.8.1. Account Reaper Background and History

At first, a simple approach of deleting an account through completely external calls was considered as it required no changes to the system. 
All data would simply be deleted in the same way the actual user would, through the public ReST API. 
However, the downside was that it would use proxy resources and log everything when it didn't really need to.
Also, it would likely need a dedicated server or two, just for issuing the delete requests.

A completely bottom-up approach was also considered, where the object and container servers would occasionally scan the data they held and check if the account was deleted,
removing the data if so. The upside was the speed of reclamation with no impact on the proxies or logging, 
but the downside was that nearly 100% of the scanning would result in no action creating a lot of I/O load for no reason.

A more container server centric approach was also considered, where the account server would mark all the containers for deletion and 
the container servers would delete the objects in each container and then themselves. This has the benefit of still speedy reclamation for accounts with a lot of containers,
but has the downside of a pretty big load spike. The process could be slowed down to alleviate the load spike possibility, 
but then the benefit of speedy reclamation is lost and what's left is just a more complex process. Also, scanning all the containers for those marked for deletion 
when the majority wouldn't be seemed wasteful. The db_replicator could do this work while performing its replication scan, 
but it would have to spawn and track deletion processes which seemed needlessly complex.

In the end, an account server centric approach seemed best, as described above.


4.6. 리플리케이션

OpenStack 오브젝트 스토리지 함수내 각 replica 독립적으로 이래, 그리고 클라이언트는 일반적으로 성공적인 오퍼레이션을  고려하기 위해 응답하는 노드의 간단한 majority (다수) 를 오직 요구한다.
네트워크 파티션과 같이 일시적인 실패는 분기하기 위해 replicas 빨리?? 원인일수 있다. 이 차이점은 결국 peer-to-peer replicator 프로세스를 비동기로서 조화되고, 
replicator 프로세스는 그들의 로컬 파일시스템을 가로지르고(횡단하고), 동시에 물리적인 디스크를 지난 부하를 분산하는 방식에서 오퍼레이션을 실행(이행)한다.

리프릴케이션은 일반적으로 로컬에서 원격 replicas 까지 오직 복사되어진 레코드와 파일과 함께, push 모델을 사용한다.
이것은 중요하다 왜냐하면 노드상의 데이터는 거기에 속하지 않을수도 있고 ( as in the case of handoffs and ring changes ),
replicator 는 데이터가 이것이 안으로 당겨야하는 클러스터에서 다른곳으로 어떻게 데이터가 존재하는지를 모를수 있다. ???
and a replicator can't know what data exists elsewhere in the cluster that it should pull in. 

데이터가 얻는 이것이 속하는곳으로 
이는 이것이 속하는곳으로 데이터가 얻는것을 보장하는 데이터를 포함하는 어떤 노드의 의무이다.
It's the duty of any node that contains data to ensure that data gets to where it belongs. 
Replica 배치는 링에 의해서 조정된다.

삭제들은 생성과 동시에 복사될수 있도록 시스템내에 매 삭제된 레코드 또는 파일은 tombstone(묘석,모비) 에 의해 표시된다.

replication 기간과 관계되어진, 그리고 얼마나 길게 일시적인 실패가 클러스터로 부터 노드를 제거할수 있는지.. 
일관성 윈도우로서 참조되어진 유효시간이후 이 tombstones 는 리플리케이션 프로세스에 의해 정리된다.
Tombstone cleanup 은 replica 집중성(집합함) 을 도달하기 위한 리플리케이션을 tied (조여야) 되어야 한다.

만약 replicator 가 원격 드라이버가 실패되어진것을 발견한다면, 이것은  동기화를 위해서 대안노드를 선택하기 위해서 링의  “get_more_nodes” 인터페이스를 사용할것이다.
몇몇 replicas 는 즉시 쓸수 있는 위치에 있을수 없을지라도,  replicator 는 일반적으로 하드웨어 실패의 직면에서 리플리케이션의 바라는(희망하는) 레벨을 유지할수 있다.
The replicator can generally maintain desired levels of replication in the face of hardware failures, though some replicas may not be in an immediately usable location.

리플리케이션은 액티브 개발의 지역이다. 그리고 정확함과 스피드로 잠재적인 향상으로 가득차 있음직하다.
Replication is an area of active development, and likely rife with potential improvements to speed and correctness.

replicator 의 두가지 주요 클래스들이 있다 - 어카운트와 컨테이너를 복제하는 db replicator 와 오브젝트 데이터를 복제하는 object replicator


4.6.1. 데이터베이스 리플리케이션

db 리플리케이션에 의해 실행(이행)된 첫번째 단계는 두개의 replicas 이미 매치가 되었는지 아닌지를 알아내기 위한 low-cost hash 비교 이다.
이 확인은 시스템내에 대부분의 데이터베이스가 아주 빠르게 이미 동기화 되는것을 검증하는것이 가능하다.
만약 해쉬들이 다르다면, replicator 는  최근 sync 포인트 이래로 더해진 레코드를 공유하므로서 sync 에서 데이터베이스를 가져온다.

이 sync 포인트는 sync 내에 있기 위해 알려진 두개의 데이터베이스에서 최근 레코드를 노팅하는 최고 수위의 표시이다. 그리고, 원격의 데이터베이스 id 와 레코드 id 의 튜플로스 각 데이터베이스에
저장된다. 데이터베이스 ids 는 데이터베이스의 모든 replicas 사이에서 유일하고, 레코드 ids 는 단조롭게(monotoically) 정수를 증가중이다. 모든 새로운 레코들이 원격 데이터베이스로 push 되어진 이
후, 로컬 데이터베이스의 전체 sync 테이블은 push 되고, 그래서 원격 데이터베이스는 로컬 데이터베이스가 이전에 동기화 디어진 모든사람과 함께 sync 내에 이것의 지금을 안다??
the entire sync table of the local database is pushed, so the remote database knows it's now in sync with everyone the local database has previously synchronized with.

만약 replica 가 전체적으로 missing 되어진것을 찾았다면, 모든 로컬 데이터페이스 파일은 rsync(1) 를 사용해서 peer 로 전송되고, 새 유일한 아이디로 소유가 확정된다. (vested )

실제는, DB 리플리케이션은 초당 동시 설정당 수백개의 데이터베이스를 처리한다 ( 사용가능한 CPU 와 디스크의 수에 달려있다 ) 그리고
실행(이행)되어져야 하는 DB 트랜잭션의 수에 의해서 바운드 된다.


4.6.2.오브젝트 리플리케이션

오브젝트 리플리케이션의 초기 구현은 단순하게   이것이 살아가기 위해 예상되어진 모든 로컬 파티션에서 모든 원격서버 까지 데이터를 push 하기 위해서 rsync 를 실행(이행)하는것이다.
이것이 작은 스케일에서 충분히(적절히,그만그만하게) 실행(이행)되는 동안, 한번 디렉토리 구조를 급등된 리플리케이션 시간은 RAM 에서 더 이상 잡을수 없다. ??
While this performed adequately at small scale, replication times skyrocketed once directory structures could no longer be held in RAM. 

우리는 지금 각 suffix 디렉토리를 위해 컨텐츠의 해쉬는 파티션당 해쉬 파일로 저장되어진 이 스키마의 수정본을 사용한다.
suffix 디렉토리를 위한 해쉬는 그 suffix 디렉토리의 컨텐츠가 수정될때 무효된다.

오브젝트 리플리케이션 프로세스는 어떤 무효된 해쉬를 계산한 이 해쉬 파일에서 읽는다. 이것은 그리고 나서 파티션을 잡고 있어야 하는 각 원격서버로 해쉬를 전송(transmit) 하고,
원격서버에서 다른 해쉬로 오직 suffix 디렉토리를 rsync 된다. 원격서버로 파일을 push 후에 리플리케이션 프로세스는 rsynced suffix 디렉토리를 위해 해쉬를 다시 계산하기 위해서
이를 공지(notifies) 한다.

오브젝트 리플리케이션의 성능은 일반적으로 이것이 가로질러야 하는 캐쉬되지 않은 디렉토리들의 수에 의해서 바운드된고, 통상 무효된 suffix 디렉토리 해쉬들의 결과로서...
우리의 구동중인 시스템으로 부터 파티션 카운트와 쓰기 볼륨을 사용하는것은, 보통 노드상에서 해쉬 공간의 2% 주의로  1일당 무효되어질수 있도록 설계되었다.
실험적으로 우리에게 허용할 수 있는 리플리케이션 스피드를 주어진것을...

Performance of object replication is generally bound by the number of uncached directories it has to traverse, usually as a result of invalidated suffix directory hashes. 
Using write volume and partition counts from our running systems, it was designed so that around 2% of the hash space on a normal node will be invalidated per day,
which has experimentally given us acceptable replication speeds.


4.7. 큰 오브젝트 관리하기 ( 5GB 보다 큰 )

OpenStack 오브젝트 스토리지는 단일 업로드 오브젝트의 크기 사이즈에서 제한이 있다; 기본값으로 이는 5GB 이다.
그러나 싱글 오브젝트의 다운로드 크기는 가상적으로 세그먼트의 개념으로 무제한된다.
더 큰 오브젝트의 세그먼트는 업로드 되고 특별한 manifest 파일은 그것이 생성된다, 다운로드 될때, 단일 오브젝트로서 연관된 모든 세그먼트를 보낸다.
이는 또한 세그먼트의 병렬 업로드의 가능성으로 업로드 스피드를 더 크게 제공한다.

OpenStack Object Storage has a limit on the size of a single uploaded object; by default this is 5GB. 
However, the download size of a single object is virtually unlimited with the concept of segmentation. 
Segments of the larger object are uploaded and a special manifest file is created that, when downloaded, sends all the segments concatenated as a single object. 
This also offers much greater upload speed with the possibility of parallel uploads of the segments.


4.7.1. Using st to Manage Segmented Objects

The quickest way to try out this feature is use the included st OpenStack Object Storage Tool. 
You can use the -S option to specify the segment size to use when splitting a large file. For example:

st upload test_container -S 1073741824 large_file

This would split the large_file into 1G segments and begin uploading those segments in parallel. 
Once all the segments have been uploaded, st will then create the manifest file so the segments can be downloaded as one.

So now, the following st command would download the entire large object:

st download test_container large_file

st uses a strict convention for its segmented object support. In the above example it will upload all the segments into a second container named test_container_segments.
These segments will have names like large_file/1290206778.25/21474836480/00000000, large_file/1290206778.25/21474836480/00000001, etc.

The main benefit for using a separate container is that the main container listings will not be polluted with all the segment names. 
The reason for using the segment name format of <name>/<timestamp>/<size>/<segment> is so that an upload of a new file with the same name won't overwrite the contents of the first
until the last moment when the manifest file is updated.

st will manage these segment files for you, deleting old segments on deletes and overwrites, etc. You can override this behavior with the --leave-segments option if desired; this is useful 
if you want to have multiple versions of the same large object available.


4.7.2. Direct API Management of Large Objects

You can also work with the segments and manifests directly with HTTP requests instead of having st do that for you. You can just upload the segments like you would any other object and 
the manifest is just a zero-byte file with an extra X-Object-Manifest header.

All the object segments need to be in the same container, have a common object name prefix, and their names sort in the order they should be concatenated. 
They don't have to be in the same container as the manifest file will be, which is useful to keep container listings clean as explained above with st.

The manifest file is simply a zero-byte file with the extra X-Object-Manifest:<container>/<prefix> header, 
where <container> is the container the object segments are in and <prefix> is the common prefix for all the segments.

It is best to upload all the segments first and then create or update the manifest. In this way, the full object won't be available for downloading until the upload is complete. 
Also, you can upload a new set of segments to a second location and then update the manifest to point to this new location. 
During the upload of the new segments, the original manifest will still be available to download the first set of segments.

Here's an example using curl with tiny 1-byte segments:


            # First, upload the segments
curl -X PUT -H 'X-Auth-Token: <token>' \
    http://<storage_url>/container/myobject/1 --data-binary '1'
curl -X PUT -H 'X-Auth-Token: <token>' \
    http://<storage_url>/container/myobject/2 --data-binary '2'
curl -X PUT -H 'X-Auth-Token: <token>' \
    http://<storage_url>/container/myobject/3 --data-binary '3'

# Next, create the manifest file
curl -X PUT -H 'X-Auth-Token: <token>' \
    -H 'X-Object-Manifest: container/myobject/' \
    http://<storage_url>/container/myobject --data-binary ''

# And now we can download the segments as a single object
curl -H 'X-Auth-Token: <token>' \
    http://<storage_url>/container/myobject


4.7.3. Additional Notes on Large Objects

With a GET or HEAD of a manifest file, the X-Object-Manifest: <container>/<prefix> header will be returned with the concatenated object so you can tell where it's getting its segments from.

The response's Content-Length for a GET or HEAD on the manifest file will be the sum of all the segments in the <container>/<prefix> listing, dynamically. 
So, uploading additional segments after the manifest is created will cause the concatenated object to be that much larger; there's no need to recreate the manifest file.

The response's Content-Type for a GET or HEAD on the manifest will be the same as the Content-Type set during the PUT request that created the manifest.
You can easily change the Content-Type by reissuing the PUT.

The response's ETag for a GET or HEAD on the manifest file will be the MD5 sum of the concatenated string of ETags for each of the segments in the <container>/<prefix> listing, dynamically. 
Usually in OpenStack Object Storage the ETag is the MD5 sum of the contents of the object, and that holds true for each segment independently.
But, it's not feasible to generate such an ETag for the manifest itself, so this method was chosen to at least offer change detection.



4.7.4. Large Object Storage History and Background

Large object support has gone through various iterations before settling on this implementation.

The primary factor driving the limitation of object size in OpenStack Object Storage is maintaining balance among the partitions of the ring. 
To maintain an even dispersion of disk usage throughout the cluster the obvious storage pattern was to simply split larger objects into smaller segments, 
which could then be glued together during a read.

Before the introduction of large object support some applications were already splitting their uploads into segments and re-assembling them on the client side after retrieving the individual pieces.
This design allowed the client to support backup and archiving of large data sets, but was also frequently employed to improve performance or reduce errors due to network interruption. 
The major disadvantage of this method is that knowledge of the original partitioning scheme is required to properly reassemble the object, which is not practical for some use cases, such as CDN origination.

In order to eliminate any barrier to entry for clients wanting to store objects larger than 5GB, initially we also prototyped fully transparent support for large object uploads.
A fully transparent implementation would support a larger max size by automatically splitting objects into segments during upload within the proxy without any changes to the client API. 
All segments were completely hidden from the client API.

This solution introduced a number of challenging failure conditions into the cluster, wouldn't provide the client with any option to do parallel uploads, and had no basis for a resume feature. 
The transparent implementation was deemed just too complex for the benefit.

The current “user manifest” design was chosen in order to provide a transparent download of large objects to the client and still provide the uploading client a clean API to support segmented uploads.

Alternative “explicit” user manifest options were discussed which would have required a pre-defined format for listing the segments to “finalize” the segmented upload. While this may offer some potential advantages, 
it was decided that pushing an added burden onto the client which could potentially limit adoption should be avoided in favor of a simpler “API” (essentially just the format of the ‘X-Object-Manifest' header).

During development it was noted that this “implicit” user manifest approach which is based on the path prefix can be potentially affected by the eventual consistency window of the container listings, 
which could theoretically cause a GET on the manifest object to return an invalid whole object for that short term. 
In reality you're unlikely to encounter this scenario unless you're running very high concurrency uploads against a small testing environment which isn't running the object-updaters or container-replicators.

Like all of OpenStack Object Storage, Large Object Support is living feature which will continue to improve and may change over time.



4.8. Throttling Resources by Setting Rate Limits

Rate limiting in OpenStack Object Storage is implemented as a pluggable middleware that you configure on the proxy server. 
Rate limiting is performed on requests that result in database writes to the account and container sqlite dbs. 
It uses memcached and is dependent on the proxy servers having highly synchronized time. 
The rate limits are limited by the accuracy of the proxy server clocks.



4.8.1. Configuration for Rate Limiting

All configuration is optional. If no account or container limits are provided there will be no rate limiting. Configuration available:

Table 4.20. Configuration options for rate limiting in proxy-server.conf file
Option	                        Default	Description
clock_accuracy	                1000	Represents how accurate the proxy servers' system clocks are with each other. 1000 means that all the proxies' clock are accurate to each other within 1 millisecond. No ratelimit should be higher than the clock accuracy.
max_sleep_time_seconds	60	App will immediately return a 498 response if the necessary sleep time ever exceeds the given max_sleep_time_seconds.
log_sleep_time_seconds	0	To allow visibility into rate limiting set this value > 0 and all sleeps greater than the number will be logged.
account_ratelimit         	0	If set, will limit all requests to /account_name and PUTs to /account_name/container_name. Number is in requests per second
account_whitelist	        ‘'	Comma separated lists of account names that will not be rate limited.
account_blacklist	                ‘'	Comma separated lists of account names that will not be allowed. Returns a 497 response.
container_ratelimit_size	        ‘'	When set with container_limit_x = r: for containers of size x, limit requests per second to r. Will limit GET and HEAD requests to /account_name/container_name and PUTs and DELETEs to /account_name/container_name/object_name

The container rate limits are linearly interpolated from the values given. A sample container rate limiting could be:

container_ratelimit_100 = 100

container_ratelimit_200 = 50

container_ratelimit_500 = 20

This would result in

Table 4.21. Values for Rate Limiting with Sample Configuration Settings
Container Size	Rate Limit
0-99	No limiting
100	100
150	75
500	20
1000	20




4.9. Managing OpenStack Object Storage with ST

In the Object Store (swift) project there is a tool that can perform a variety of tasks on your storage cluster named st. This client utility can be used for adhoc processing, to gather statistics, 
list items, update metadata, upload, download and delete files. It is based on the native swift client library client.py. 
Incorporating client.py into st provides many benefits such as seamlessly re-authorizing if the current token expires in the middle of processing, 
retrying operations up to five times and a processing concurrency of 10. All of these things help make the st tool robust and great for operational use.

4.9.1. ST Basics

The command line usage for st is:

st (command) [options] [args]

Here are the available commands for st.

stat [container] [object]

Displays information for the account, container, or object depending on the args given (if any).

list [options] [container]

Lists the containers for the account or the objects for a container. -p or -prefix is an option that will only list items beginning with that prefix.
-d or -delimiter is option (for container listings only) that will roll up items with the given delimiter, or character that can act as a nested directory organizer.

upload [options] container file_or_directory [file_or_directory] […]

Uploads to the given container the files and directories specified by the remaining args. -c or -changed is an option that will only upload files that have changed since the last upload.

post [options] [container] [object]

Updates meta information for the account, container, or object depending on the args given. If the container is not found, it will be created automatically; 
but this is not true for accounts and objects. Containers also allow the -r (or -read-acl) and -w (or -write-acl) options. 
The -m or -meta option is allowed on all and used to define the user meta data items to set in the form Name:Value. This option can be repeated.

Example: post -m Color:Blue -m Size:Large

download ?all OR download container [object] [object] …

Downloads everything in the account (with ?all), or everything in a container, or a list of objects depending on the args given. 
For a single object download, you may use the -o [?output] (filename) option to redirect the output to a specific file or if “-” then just redirect to stdout.

delete ?all OR delete container [object] [object] …

Deletes everything in the account (with ?all), or everything in a container, or a list of objects depending on the args given.

Example: st -A https://auth.api.rackspacecloud.com/v1.0 -U user -K key stat

Options for st

-version show program’s version number and exit

-h, -help show this help message and exit

-s, -snet Use SERVICENET internal network

-v, -verbose Print more info

-q, -quiet Suppress status output

-A AUTH, -auth=AUTH URL for obtaining an auth token

-U USER, -user=USER User name for obtaining an auth token

-K KEY, -key=KEY Key for obtaining an auth token




4.9.2. Analyzing Log Files with ST

When you want quick, command-line answers to questions about logs, you can use st with the -o or -output option. 
The -o ?output option can only be used with a single object download to redirect the data stream to either a different file name or to STDOUT (-). 
The ability to redirect the output to STDOUT allows you to pipe “|” data without saving it to disk first. 
One common use case is being able to do some quick log file analysis. First let’s use st to setup some data for the examples.
The “logtest” directory contains four log files with the following line format.

files:
                2010-11-16-21_access.log
                2010-11-16-22_access.log
                2010-11-15-21_access.log
                2010-11-15-22_access.log
                
                log lines:
                Nov 15 21:53:52 lucid64 proxy-server - 127.0.0.1 15/Nov/2010/22/53/52 DELETE /v1/AUTH_cd4f57824deb4248a533f2c28bf156d3/2eefc05599d44df38a7f18b0b42ffedd HTTP/1.0 204 - - test%3Atester%2CAUTH_tkcdab3c6296e249d7b7e2454ee57266ff - - - txaba5984c-aac7-460e-b04b-afc43f0c6571 - 0.0432

The st tool can easily upload the four log files into a container named “logtest”:



                $ cd logs
                $ st -A http://swift-auth.com:11000/v1.0 -U test:tester -K \
                testing upload logtest *.log
                2010-11-16-21_access.log
                2010-11-16-22_access.log
                2010-11-15-21_access.log
                2010-11-15-22_access.log
                
                get statistics on the account:
                $ st -A http://swift-auth.com:11000/v1.0 -U test:tester -K \
                testing -q stat
                Account: AUTH_cd4f57824deb4248a533f2c28bf156d3
                Containers: 1
                Objects: 4
                Bytes: 5888268
                
                get statistics on the container:
                $ st -A http://swift-auth.com:11000/v1.0 -U test:tester -K \
                testing stat logtest
                Account: AUTH_cd4f57824deb4248a533f2c28bf156d3
                Container: logtest
                Objects: 4
                Bytes: 5864468
                Read ACL:
                Write ACL:
                
                list all the objects in the container:
                $ st -A http:///swift-auth.com:11000/v1.0 -U test:tester -K \
                testing list logtest
                2010-11-15-21_access.log
                2010-11-15-22_access.log
                2010-11-16-21_access.log
                2010-11-16-22_access.log

These next three examples use the -o ?output option with (-) to help answer questions about the uploaded log files. 
The st command will download an object, stream it to awk to determine the breakdown of requests by return code for everything during 2200 on November 16th, 
2010. Based on the log line format column 9 is the type of request and column 12 is the return code. 
After awk processes the data stream it is piped to sort and then uniq -c to sum up the number of occurrences for each combination of request type and return code.

$ st -A http://swift-auth.com:11000/v1.0 -U test:tester -K \
                    testing download -o - logtest 2010-11-16-22_access.log \
                    | awk ‘{ print $9”-“$12}’ | sort | uniq -c
                    
                    805 DELETE-204
                    12 DELETE-404
                    2 DELETE-409
                    723 GET-200
                    142 GET-204
                    74 GET-206
                    80 GET-304
                    34 GET-401
                    5 GET-403
                    18 GET-404
                    166 GET-412
                    2 GET-416
                    50 HEAD-200
                    17 HEAD-204
                    20 HEAD-401
                    8 HEAD-404
                    30 POST-202
                    25 POST-204
                    22 POST-400
                    6 POST-404
                    842 PUT-201
                    2 PUT-202
                    32 PUT-400
                    4 PUT-403
                    4 PUT-404
                    2 PUT-411
                    6 PUT-412
                    6 PUT-413
                    2 PUT-422
                    8 PUT-499
                    

This example uses a bash for loop with awk, st with its -o ?output option with a hyphen (-) to find out how many PUT requests are in each log file. 
First create a list of objects by running st with the list command on the “logtest” container; 
then for each item in the list run st with download -o - then pipe the output into grep to filter the put requests and finally into wc -l to count the lines.

$ for f in `st -A http://swift-auth.com:11000/v1.0 -U test:tester -K testing list logtest` ; \
                        do  echo -ne “PUTS - ” ; st -A http://swift-auth.com:11000/v1.0 -U test:tester -K \
                        testing download -o -  logtest $f | grep PUT | wc -l ; done
                        
                        2010-11-15-21_access.log - PUTS - 402
                        2010-11-15-22_access.log - PUTS - 1091
                        2010-11-16-21_access.log - PUTS - 892
                        2010-11-16-22_access.log - PUTS - 910
                        

By adding the -p ?prefix option a prefix query is performed on the list to return only the object names that begin with a specific string. 
Let’s determine out how many PUT requests are in each object with a name beginning with “2010-11-15”. 
First create a list of objects by running st with the list command on the “logtest” container with the prefix option -p 2010-11-15. 
Then on each of item(s) returned run st with the download -o - then pipe the output to grep and wc as in the previous example. 
The echo command is added to display the object name.

$ for f in `st -A http://swift-auth.com:11000/v1.0 -U test:tester -K testing list  \
                        -p 2010-11-15 logtest` ; do  echo -ne “$f - PUTS - ” ;  \
                        st -A http://127.0.0.1:11000/v1.0 -U test:tester -K testing  \
                        download -o - logtest $f | grep PUT | wc -l ; done
                        
                        2010-11-15-21_access.log - PUTS - 402
                        2010-11-15-22_access.log - PUTS - 910
                        
                        

The st utility is simple, scalable, flexible and provides useful solutions all of which are core principles of cloud computing; 
with the -o output option being just one of its many features.






# proxy node 에서 rebalance 시 아래와 같이 오류가 나타날시에는 snode 를 한대로 구성시 나타남.
IndexError: list index out of range


# Swift auth node 설치
apt-get install swift-auth
vi /etc/swift/auth-server.conf
    [DEFAULT]
    cert_file = /etc/swift/cert.crt
    key_file = /etc/swift/cert.key
    user = swift

    [pipeline:main]
    pipeline = auth-server

    [app:auth-server]
    use = egg:swift#auth
    default_cluster_url = https://<PROXY_HOSTNAME>:8080/v1
    super_admin_key = devauth       //--- devauth는 다른 것으로 변경할 것

swift-init auth start
chown swift:swift /etc/swift/auth.db
swift-init auth restart


Swift Storage Node 설치